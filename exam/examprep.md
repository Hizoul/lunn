# Neural Network Exam Prep

## Intro
- History of "classical" NNs
	- Pitts & McCulloch => Neuron Definition
	- Rosenblatt => Convergence Theorem
	- Widrow => Kind of Backpropagation
	- LeCuns first conv nn 1989 for AT&T digit recognition
	- 2006 => restricted boltzmann machine building block for DNN
	- 2013 => deepminds alpha go zero etc.
- Early applications (NetTalk / Alvin)
	- 1987 nettalk => reads text aloud
	- 1989 => ALVINN autonomous driving with 30x32px camera
- Enabling factors for the "Deep Learning" revolution
	- le cuns OCR 1989
	- mnist / imagenet perviously these big datasets where not available
	- more poreful hardware through gpu (and soon tpus)
	- new algorithms and architectures (not limited to MLP)
	- many other applications suddenly beating existing approaches (deep belief network, stacked auto-encoder, deep recurrent network)
- Why more layers
	- one layer sufficient for any function in theory
	- number of nodes and weights grow exponentially
	- consecutive layers "learn" more complicated features building upon the previous ones
- Traditional Machine Learning Problems
	- Classification => predict class label
	- regression => predict a value
	- clustering => find clusters in data
	- associations => find frequent patterns in data
- to adjust results one can
	- increase the bias
	- increase the weights
	- change the result of the previous activation by changing their weights
- hebbian theory = neurons that fire together wire together

## Statistical Pattern Recognition
- Bayes Theorem
	- probability of an event based on prior knowledge of conditions
	- P(A | B) => likelyhood of A if B is true
	- $P(A|B) = \frac{P(B|A) * P(A)}{P(B)}$
	- enables estimation of posteriror probabilities
- Optimal Decision Boundary
	- joint probability densities
	- e.g. 2 classes. each of them produces a classification value
	- draw bar graphs in different color per class
	- find lowest overlap for minimal amount of misclassifications
	- best fit is where they cross
- Loss Matrix and Risk Minimization
	- Risk => cost of misclassification
	- use loss matrix to quantify cost
	- use the one with smallest loss
	- cost => if higher cost for one parameter it's change impact is higher!
- recheck "apple banana"
- Overfitting
	- usually means training set works well but test set doesn't
	- more parameters => higher risk of overfitting
- Regularization
	- prevent overfitting by imposing constraints on values or number of model parameters
	- done by e.g. penalizing large cofficient values (shrinkage ridge regression weight decay)
- Cross-validation
	- monitor error botn in training and test set

## Linear Models
- Linear separability
	- two classes are linearly seperable if there exists a hyperplane that separates them
	- formulas always need to be bigger than each other
	- doesn't imply convexness
- Cover's Theorem (1965)
	- what is the chance that randomly labeled set of N Points in d-dimensional space is linearly seperable?
	- if number of points in d dimensional space smaller than 2*d then probably seperable
	- $= \frac{N}{d-1}$ => 500 images with 16*16px => d = 16*16; N = 500 =>$\frac{500}{257} \approx 2$
- Perceptron Learning Algo
	- find suitable values for wheights such that training set gets classified correctly
	- geometrically => find hyper-plane that separates two classes
	- works by => initialize weight randomly; for each misclassified sample adjust weights
	- convergence => if linearly seperable the learning will terminate successfully after a finite number of iterations
- Gallant's Pocket Algo
	- initial idea => see if weight change improved => expensive to recheck
	- improvement => count consecutive correct classifications
	- if best run then keep weights saved for later
	- converges with probability 1 BUT might be multiple separating hyperplanes and it picked a bad one
	- if around 2 probably seperable
- Adaline
	- main idea => minimize Mean Squared Error
	- supports arbitrary values (not limited to -1 & 1)
	- can solve linear regression
- Logistic Regression
	- replace sign function by smooth approximation and use steepest descent to find weights that minize error
- Stochastic Gradient Descent
	- finds minimum of a function (e.g. error function)
	- works faster
	- better avoidance of bad local minima
- Multi-class linear separability
	- there exists c linear discriminant functions such that each x is assigned to a class
- Multi-class Perceptron (Slide 44)
- Histogram
	- data binning leads to loss of information
	- number of buckets increases exponentially with data dimensionality (feature amount) hence amount of records to classify also exponentially grows

## MLP and Backpropagation
- MLP (Multi-layer Perceptron)
	- single perceptrons => linear decision boundary => XOR unsolvable
	- via multi layers this is overcome
	- require training through backpropagation
- Expressive Power of MLP (Slide 5)
	- permutation invariant because no convolution applied
	- each layer allows for more convex regions to be drawn for classification
	- single node => single line
	- one hidden layer => half plane bounded by hyperplane
	- two hidden layers => convex open / closed regions
	- three layer => arbitrary shapes
	- every boolean function can be described by single hidden layer
	- continous functions any bounded continuous function can be approximated by two hidden layers
- Backpropagation Algo
	- forward pass => in this step the network is activated on one example and the error of each neuron of the output layer is computed
	- backward pass => in this step the network error is used for updating the weights. Starting at the output layer, the error is propagated backwards through the network, layer by layer with help of the generalized delta rule. Finally, all weights are updated
	- doesn't guarranteee convergence, weights initialized randomly
	- backpropagation => find wanted weight adjustment for a single example
	- for each example look at activation result for each neuron and desired output
	- accumulate changes to previous weights
	- recursively do so until reach start
	- average together desired changes of each example => negative gradient of the cost function
	- because very expenssive following is done do stochastig gradient descent
		- random order of training samples put together into mini-batches
		- for each mini-patch do backprop => approximation
    - disatvantages
    	- no convergence guaranee
    	- only local minimum found
- Backpropagation update modes
	- batch mode => all inputs at once, takes longest
	- (stochastic gradient descent) mini-batch mode => use small random subsamples to process => quicker but approximated
	- on-line mode => update weights using one example at a time
- Backpropagation when to stop
	- total mean squared error change => when MSE change is sufficiently small
	- when reaching certain percentage for training or test set (helps against overfitting)
- Alternative Error Messures
	- (SSE) for regression problems use linear outputs and sum quared error
	- (Logistic) for binary classification use logistic output unit and minimze cross entropy
	- (Cross Entropy + Softmax) for multiclass use softmax and minimize cross entropy


## Convolutional Networks
- On more Layers
	- adding more layers is harmful because
		- lack of generalization (increased number of weights quickly overfits data)
		- huge number of (bad) local minima to trap the gradient descent algo in
		- vanishing or exploding gradients (update rule involves products of many numbers) cause additional problems
- Convolution
	- convolve one matrix into a smaller one by applying a so called kernel
	- results in "degree of overlap"
	- helps "feature detectors" => returns high values when corresponding patch is similar to filter matrix
- bias => every operation (e.g. 5x5 conv has ONE bias so 5x5+1)
- Weight Sharing (???)
	- weights in convolutional layers are shared to reduce memory and improve performance
- Subsampling (???)
- local receptive fields (???)
- Pooling
	- progressively reduce spatial size of representation
	- reduce amount of features and hence complexity
	- reduces e.g. 3x3 to 2x2 etc.
	- Reduction of resolution (and size) of feature maps, enforcing generalization by losing some information about location of features, filtering out noise.
- dropout
	- at each training stage node can be dropped with a probability
	- for altering weights ignore dropped nodes but reinsert htem after
	- by not changing all nodes chances of overfitting are decreased#
	- During each cycle of the training process a fraction of neurons (e.g., 50%) is disabled and corresponding weights are not used or affected by the training algorithm. It is a powerful technique of preventing overfitting by reduction of complex co-adaptations of neurons and forcing network to learn more robust features that are useful in conjunction with many different random subsets of the other neurons.
- ReLU's (???)
	- alternative to sigmoid, hyperbolic tangent etc.
	- increases non-linear properties of decision function
	- preferred over sigmoid etc. because it trains faster without big accuracy changes
- Be able to count weights between layers
	- filter verkleinerung => Filtergröße - 1 bsp. 32 orig 5x5 => 32-4 = 28
	- Shared Weights => (FilterGrößer*FilterGröße + 1) * amount of feature maps
	- Connections => 28x28 + FilterGröße*FilterGröße+1
	- fully connected => (input*input+1)*(reduced*reduced)*amount of feature maps
- Which architectural ideas do Convolutional Neural Networks combine to ensure some degree of shift, scale and distortion invariance?
	- local receptive fields
	- shared weights
	- subsampling

## GPU learning
- know theano tensorflow keras
- limit gpu

## Recurrent Neural Networks
- made for sequential data
	- one to one ( no recurrence)
	- one to many (e.g. image captioning)
	- many to one (e.g. sentiment analysis)
	- many to many
		- sequence input to output e.g. machine translation
		synced seqnece input and output e.g. classification of video frames
- RNN (plain)
	- problems
		- exploding vanishing gradients
		- speed at which past actions are forgotten
	- make use of sequential information
	- traditionally all inputs and outputs independent of each other
	- called recurrent cause same task for every element of sequences is done
	- plain rnns would use backpropagation through time
		- unfold network over time+
		- show the first word compute hidden state and error
		- show second word, compute hidden state and error etc.
		- $error(w1,....,wk)=F(U, W, V, w1..., wk)$
		- use SDG for minimum
	- because longer sequence => longer network and more problems with error poropagation limited to 5-10 sequences but lstm helps
	- for new hidden state combine current input with previous state where f is activation (sigmoid) function $f(Uw(t)+Ws(t-1)$
	- output = softmax of new state
- LSTM (Long Short Term Memory) Networks
	- let the network decide which info requires preserving
	- kept in a cell state vector
	- hidden layer replaced by specially designed network
	- consists of 3 gates (Explain architecture of LSTM Layer)
		- forget gate => decides whcih info of state should be neglected (sigmoid takes 0,1 activation)
		- input gate => decide which info should be added to internal state (tanh generates new info; sigma filters it)
		- output gate => computes the final output by combining new input, previous output and cell state
- GRU basically LSTM but simplified => no cell state
- Leading example of text generation
	- output and input layer size = amount of words as vector
	- output uses softmax function hence it is a probability distribution
- Understand difference between stateful and stateless mode
	- stateless => good for translation because each data point is not interested in the previous one
	- stateful => good for text gen etc. because info from previous sentences important
	- *stateful* => reuse cell states from last run / after batch is processed DON'T reset network state
	- *stateless* => don't reuse cell states from last run / reset network state after batch process
- Know various usages of RNNs (slide 5)
	- image captioning
	- sentiment analysis
	- machine translation
	- video frame classification

## Autoencoders
- features / characteristics
	- lossy encoding
	- unsupervised encoding
	- data dependent => tree encoder can't classify houses
- Stacked => stacked sparse autoencoder layers are used for image recognition
- sparsity => if node has activation lower than X set it to 0, leads to lower values in the encoded state (e.g. might lead to less data if instead of long can use int)
- Denoising
	- train network with broken to healed pictures
	- apply result => works quite well
- Other Applications
	- dimensionality reduction / compression 
	- varational autoencoder
		- train regular autoencoder
		- project it onto latent space (nice clusters)
		- combining latent space and learned features it can generate new images
	- classifying / clustering text documents
	- sparse => reducing dimensionality in a lossy way
	- the encoded values can be used for comparison and searching for similarities
- deep autoencoder
	- first half uses unsupervised dbns
	- upper part is mirror of lower part hence not rbm because doesn't go backward again and uses its own independent weights
	- usually deep autoencoders very difficult to optimize via backrpopagtion:
		- train stack of 4 rbms; unroll them; fine-tune via backpropagation

## Optimization
- Learning as optimization => mimize expected loss over traning dataset
- Gradient descent
	- classic gradient descent => always points in the direction of steepest increase
	- stochastic gradient descent => use small random batches for faster calculation
		- faster
		- avoids local minima
		- better generalization, prevents overfitting => kind of regularization
	- momentum => dampened oscillations and faster convergence
	- nesterov accelerated gradient
		- accounts future momentum
		- in practice slightly better than momentum
- Batch Normalization
	- normalize both input and output at every layer by substracting the batch mean and dividing by batch standard deviation
	- adds two trainable parameters PER LAYER => scale Gamma and shift Beta, these two influence two other parameters called standard deviation and mean
	- reduces the amount by which hidden unit values shift around (covariance shift)
	- helps independence between layers
	- enables higher learning rates because activations don't go really high or low anymore
	- reduces overfitting through its regularization effects (similar to dropout but less aggressive
	- prevents exploding or vanishing gradients
	- easier to get out of local minima
- Be able to calculate diretion (a = learning rate; gradient )
	- SGD => $p_{new} = p_{old} - a * gradient$
	- Momentum => $p_{new} = p_{old} - a * gradient + b * last direction$
	- Nesterov => $p_{new} = momentumRes + b * (momentumRes - momentumResPrev)$

## RBMs and Deep Belief Networks
NEW
- Restricted Boltzmann Machine
	- 2 layered unsupervised network
	- nodes have binary values (0 or 1)
	- for classification uses forward AND backward pass!
	- trains unlabeled data
	- automatically extracts useful features from unlabeled input
	- has data dependent regularlizer that is independent of labels $-log(p(x))$
	- energy function => 
	- training purpose => maxime log likelihood of x
- contrastive divergence
	- gradient descent with a trick => approximate second term by iterating a random process for a few times (found by hinton)
	- The three passes up, down, and up require 3mn multiplications, updating weights (calculation of x0h0-x1h1) requires 2mn more multiplications; thus in total 5mn multiplications are required.
- Deep belief networks
	- greedily stack rbms
	- although "unsupervised" learning requires a small labeled data set to associate learned features with the actual labels
	- somehow solves vanishing gradient problem
	- attach a feed forward neural network to associate learned features with labels
-------
- Restricted Boltzmann Machine
	- 2 layered unsupervised network
	- nodes take values 0 or 1
	- model of probability distribution of P(x, h)
	- Energy, generative model, likelihood
	- train via gradient descent
	- used for (recommendations, feature extraction, dimensionality reduction, collaborative filtering)
	- optimized using log likelihood during training
	- as heuristic use constrastive divergence
- Contrastive Divergence Algo
	- training undirected graphical models
	- relies on approximation of gradient
	- traditional inefficient approach to train a RBM: up, down, up, down ... infinity long, then w_new = learning_rate * (visibleLayer_initial * hiddenLayer_initial - visibleLayer_inf * hiddenLayer_inf)
	- with CD: w_new = learning_rate * (visibleLayer_initial * hiddenLayer_initial - visibleLayer_1 * hiddenLayer_1)
- doesn't maximize log likelihood anymore but still works
	
- Deep Belief Networks
	- same structure as classical MLP only way of training different
	- stack RBMs in a greedy manner
	- extract deep hierarchical representations of training data
	- uses unlabeled data
- Auto Encoders
	- use to cluster topcis
- Netflix RBM challenge
	- 1mil price money, 100000000 records, 5000000 customers 18000 movies, each has a rating
	- blend together different models for optimal results
	- calculation includes Ratings*Movies*Hiddenlayers NOT USERS! they are data

## Convolutional Networks 2
- scale up networks => use sparse interactions
	- sparesly connected (not every neuron connected to every one of the next layer)
- tiled convolution => cycle between groups of shared parameters
- AlexNet
	- 5 convs 3 fully connected layers and 1000way softmax => 8 layers in total
	- local response normalization
	- overlapping pooling
	- uses data augmentation
		- crop 227 to 224 and also mirror horizontally
		- change intensity of rgb channels
	- uses dropout with probability 0.5 and only in the dense fully connected layers
	- uses sdg with momentum
	- network has two pipelines so it's faster to learn with two gpus
- ResNet
	- currently best image classification net
	- 152 / 101 / 50 / 34 layer version; the deeper the better
	- generally => stacking a lot of layers doesn't improve but can worsen results
	- to solve multi layer problem:
		- use shallower model with less layers
		- insert a lot of identity layers 
		- identity layer = add input value on top of activated value
	- (almost) no max pooling, no dropout, no fully connected layer
	- uses batch normalization and data augmentation
- Treating image boundaries
	- valid => never go out of bounds
	- same => pad the input with 0 so convolution does not change resolution
	- full => pad image with sufficent number of 0s so each pixel is visited exactly k times (k = kernel size)
- Weight initialization strategies (Glorot, GLorot uniform)
	- many different possibilites
	- in / out = amount of input / output nodes
	- glorot => normally distributed with 0 mean and standard deviation $stddev=sqrt(2/in+out))$
	- glorot uniform => weights uniformly distributed between -B and +B $B=sqrt(6) / in+out$
- Data Augmentation
- Applications
	- cnn
		- diagnosing diabetis
		- cancer 
	- reinforcement
		- games
		- alpha go
- residual networks
	- work better because backpropagation can go straight through all layers using shortcuts

## Q-Learning / Go / Alpha Go Zero


## PARAMETRIC DENSITY ESTIMATION NN 3 Slide 2
- maximize product and maximize logarithm sum
- to find optimum
	- uniform distribution => a = min(X) b = MAX(X)
	- normal distribution => a = mean(X); b = std(X)
- understand discriminant functions for sets a und b

# know which activation / error functions are good for which thing
- binary classification => sigmoid + cross entropy
- multi class => cross entropy + softmax
- regression problems => linear + square error

# overfitting prevention
- dropout
- pooling
- (batch) normalization
- stochastic gradient descnet
- early stopping
- regularization L2
- dropout
- data augmentation
	- e.g. use known image and apply various filters to it like color, saturation etc.

# Q-Learning / Alpha Go (Zero)
- classical agent system
	- states, actions, rewards, policies
- state solely read through images via CNN
- data => 4 consecutive frames
- network => CNN + Q Learning = DQN
- policy => behaviour function that selects action based on state $action = \pi(s)$
- function for expected reward (value function) => $Q^\pi (s, a)$
- q learning
	- $Q(s, a, w) \approx Q^\pi(s, a)$
	- objective function via mean squared error
	- naive q-learning either oscialltes around optimum or diverges completely hence can be unstable
- deep q learning
	- target is maximizing the value function by using weights
	- experience replay => break correlation in data and learn from all past policies
		- optimize mse between q network and q learning targets
	- freeze target q network => avoid oscillations and break corellation between q network and target
		- only periodically update fixed / frozen parameters (weights)
	- reward / value range: clip rewards or normalize network adaptively (always -1 to +1), prevents gradients becoming too large
- atari dqn
	- reward is change in score for that step
- alpha go zero
	- trains entirely from self play (no previous data)
	- no handcrafted features, only observes board state
	- residual network instead of convolution
	- previously policy and value seperate networks => now combined
	- use two feature maps for white and black stones seperately
	- seven past board states as history => acts as attention mechanism
	- outputs => value = probability that it's going to win; policy = probability distribution over possible moves
	- self play usually leads to unstable results => monte carle tree search prevents unstability
	- for every turn the policy part 1600 possible future turns and use monte carle tree search on that
	- monte carle tree search improves result, but raw network already almost as good as humans
	- go is a good example because it's easy to simulate the future, every information needed is easily observable

# to learn
- bayes nicht nur umstellen sondern angepasste wahrscheinlichkeiten!
	- related to density
- log likelihood => use values from X insert into function, multiply each result. apply log. Log will get bigger with bigger numbers
	- used in RBM for optimization
	- in parametric denstiy estimation maximize => product of likelihoods, sum of logarithms of likelihood
- schätzen wie viel iterationen von gradient descent
	- $xNeu = xAlt - learningRate* f'(xAlt)$
- XOR-Problem
- if all weights are 1 or 0, changes always same => never converges because backpropagation takes derivative from previous layer
- linear seperability
- anzahl weights etc. FÜR ALLE NETZWERKE RNN, CNN etc.
	- cnn
        - filter verkleinerung => Filtergröße - 1 bsp. 32 orig 5x5 => 32-4 = 28
        - Shared Weights => (FilterGrößer*FilterGröße + 1) * amount of feature maps
        - Connections => 28x28 + FilterGröße*FilterGröße+1
        - fully connected => (input*input+1)*(reduced*reduced)*amount of feature maps
    - rnn
    	- bsp. 8000 input 100 hidden 8000 output
    	- 2 * 100 * 8000 + 100 * 100
    	- (input + output) * hidden + hidden * hidden
- contrastive divergence
	- single update of weights for single input vector => 5 * M * N
	- anything added increase the 5
	- The three passes up, down, and up require 3mn multiplications, updating weights (calculation of x0h0-x1h1) requires 2mn more multiplications; thus in total 5mn multiplications are required.
- gaussian distirbution parameters
	- means and covariances
	- amount of means = amount of features
	- amount of covariances = fakultät von features => für 10 => 10+9+8+7+6+5+4+3+2+1
	- biggest limiation => data is never normally distributed
- RBMS + contrastive divergence in detail!!!
- deep belief network
- ALLES FÜR RNNS

### DO
Batch normalization
q-learning
conv2

# Vanishing / Exploding Gradients
- simple / plain rnns
- adding a lot of layers without any sense in an MLP
- 