% https://arxiv.org/pdf/1312.6034v2.pdf - Deep Inside Convolutional Networks: Visualising Image Classification Models and Saliency Maps

% Gradient: Technik um größten Anstieg zu finden -> man nimmt also beim Trainieren den negativen Gradient: -Delta(->W) = [0.2 -1 ...] // GIbt an wie die Weights verändert werden sollten
% Numerische Approximation des Gradients: https://en.wikipedia.org/wiki/Numerical_differentiation
% Gradient Descent/ Full Backpropagation): mache es für alle Trainingsamples und nehme den durchschnitt
% Stochasitic Gradient Descent: Nehm ein paar und nehme den Durchschnitt
% Backpropagation: Determine how one trainign example want to change the weights and biases

\section{How Does Dreaming Work}
\label{sec:how}

This section will focus on the main techniques how dreaming actual work.
Similar to the training process of a network, the dreaming algorithm is an optimization problem.
But rather than training a network to classify samples, one take a pretrained network and modify the input that some neurons in the net are getting more activated.


\subsection{General}
When looking at the training process of a (Convolutional) Neural Network, one can see that the weights and biases are changed in such a way that a given error function gets minimized.
Because the error function, for instance \emph{mean squared error}, has a lot of dependencies to the weights and biases the derivation is very complex if not even infeasible.
That's why the \emph{Gradient descent} algorithm is used to find the (global) minimum\footnote{however, using the gradient descent often leads only to a local minimum} of the function by taking small steps into the direction of the minima.
The partial derivative $\frac{\partial C}{\partial w}$ in respects to the weights	$\frac{\partial C}{\partial b}$ and to the biases of the cost function C is used to calculate how quickly the cost changes when the weights and biases are changed.

So in order to learn a network there are two essential steps:
First there is the \emph{forward} pass, where a sample gets classified by the net.
After that the error made by the network is calculated with the cost function.
The second step, the \emph{backward} pass, consists of the algorithm of the Gradient Descent, often called \emph{backpropagation} in the context of Neural Networks.
Based on result of the error function, also called \emph{loss}, the gradients for every weight and bias is calculated.

The gradients can be interpret as the direction in which the function should step in order to maximize the loss.
Hence we want to minimize the loss we take the negative gradient and add them to the weights and biases.
To prevent too slow learning or \enquote{overshooting} a local minima a \emph{learning rate} is introduced which gets multiplied with the gradients.

%TODO: Vielleicht Layer als l_end beschreiben, damit man dann sowas sagen kann wie: Gradients von Layer 0 - l_end sind berechnet worden
As already said in the introduction of this section, the goal of the Deep Dream algorithm is not to minimize the cost function, but to modify the input to maximize the activations of certain neurons.
Concrete speaking one can choose a whole layer within the net or just some neurons of a layer which output should be maximized according to the L2 norm.
By a normal forward pass to the specified layer the activation values gets computed.
In order to start the backward pass now, the gradient of the specified layer has to be set.
This can be done in several ways, at this point just assume that the gradients are set to the same values as the
activation values of that layer.
By starting the backward pass the backpropagation algorithm composes the gradient of each previous layer to compute the gradient of the whole submodel by automatic differentiation.\footnote{TODO Quelle: \url{http://caffe.berkeleyvision.org/tutorial/forward_backward.html}}
That's why the gradients (or at least the values) of the specified layer has to be set to something, because every layer needs the gradients from its previous layer in order to work.


When the backward pass is done, all gradients are computed.
Obviously the gradients consists of many as values as there are weights and biases.
But with Deep Dream only the weights of the input layer are important, because these can be added to the input image.
By adding the gradients, the activation values at the specified layer will increase or decrease and with that, respectively the features will become more or less present.

\subsection{Optimizations}
\label{sec:optimizations}
Same as the normal training process of the network, the dreaming is done by using multiple iteration.
By repeating the process over and over again, the features become more and more present, obviously only to an extend.

Another thing that is commonly used is the \emph{step size}, also called \emph{learning rate} in Neural Networks.
Instead of just adding the gradients as they are, one can higher or lower the effect by choosing a $step size < 1$ and $> 1$ respectively.

Besides these two rather simple techniques \emph{octaves} are used.
The goal is of this procedure is to scale the image after every dream step.
By increasing the size of the image in such a way, the network (hopefully) tend to find new features and enhances them as well, which finally leads to even more spectacular pattern.
The algorithm starts with the smallest version of the image and do the normal dreaming process.
By calculating the difference between the original and the dreamed image the added or removed features are extracted.
For the next octave both the base image and the features of the previous octave get scaled by a scaling factor and are added up together.
With this technique it is possible to create very large images, which will contain more and more features as the process proceeds.

 
Another small tweak is to use a jitter, that shifts the image along the x and y-axis in a random interval before the forward and backward pass.
This will lead to different activation functions and hence to different gradients and finally to a different change to the image.
This jitter shift is undone, after the image is modified.
This small trick not just lead to different results, it also provides smooth transitions between remarkable enhanced features.

Because enhanced features can look harsh, especially with a high step size, a good way to smooth things out is to use a filter. The filter can either be applied after every iteration or after every octave.
In Section \ref{sec:todo} the best results are presented and the used filters mentioned.


\subsection{Guided Dreaming}
\label{guided-dreaming}
Instead of increasing the occurrence of present features within an image one can also use a reference image in order to achieve \emph{Guided Dreaming}.
The approach is to do a normal forward pass with a reference image and to save the activations at the specified layer.
During the dreaming these activations are set to the gradients at the specified layers.
Doing this leads to results, where the features of the reference image are enhanced in the input image.
If one would give a reference image of cats to the network, the Deep Dream algorithm will create/enhance the learned features of cats in the input image.

There are also different approaches of the guided dreaming.
For instance instead of just modify the input image according to the activations of the reference image, one could also select the best matching correspondences from the guide and the base image.
To achieve this, the dot-products between the saved and the current activations of the image is calculated in order to find the best matching ones.
This is done because it doesn't really make sense to enhance features in the original image, that aren't there at all.  \footnote{https://github.com/google/deepdream/blob/master/dream.ipynb}


