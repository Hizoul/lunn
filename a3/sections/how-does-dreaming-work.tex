% https://arxiv.org/pdf/1312.6034v2.pdf - Deep Inside Convolutional Networks: Visualising Image Classification Models and Saliency Maps

% Gradient: Technik um größten Anstieg zu finden -> man nimmt also beim Trainieren den negativen Gradient: -Delta(->W) = [0.2 -1 ...] // GIbt an wie die Weights verändert werden sollten
% Numerische Approximation des Gradients: https://en.wikipedia.org/wiki/Numerical_differentiation
% Gradient Descent/ Full Backpropagation): mache es für alle Trainingsamples und nehme den durchschnitt
% Stochasitic Gradient Descent: Nehm ein paar und nehme den Durchschnitt
% Backpropagation: Determine how one trainign example want to change the weights and biases

\section{How Does Dreaming Work}
\label{sec:how}

This section will focus on the main techniques how dreaming actual work.
Similar to the training process of a network, the dreaming algorithm is an optimization problem.
But rather than training a network to classify samples, one take a pretrained network and modify the input that some neurons in the net are getting more activated.


\subsection{General}
When looking at the training process of a (Convolutional) Neural Network, one can see that the weights and biases are changed in such a way that a given error function gets minimized.
Because the error function, for instance \emph{mean squared error}, has a lot of dependencies to the weights and biases the derivation is very complex if not even infeasible.
That's why the \emph{Gradient descent} algorithm is used to find the (global) minimum\footnote{however, using the gradient descent often leads only to a local minimum} of the function by taking small steps into the direction of the minima.
The partial derivative $\frac{\partial C}{\partial w}$ in respects to the weights	$\frac{\partial C}{\partial b}$ and to the biases of the cost function C is used to calculate how quickly the cost changes when the weights and biases are changed.

So in order to learn a network there are two essential steps:
First there is the \emph{forward} pass, where a sample gets classified by the net.
After that the error made by the network is calculated with the cost function.
The second step, the \emph{backward} pass, consists of the algorithm of the Gradient Descent, often called \emph{backpropagation} in the context of Neural Networks.
Based on result of the error function, also called \emph{loss}, the gradients for every weight and bias is calculated.

The gradients can be interpret as the direction in which the function should step in order to maximize the loss.
Hence we want to minimize the loss we take the negative gradient and add them to the weights and biases.
To prevent too slow learning or \enquote{overshooting} a local minima a \emph{learning rate} is introduced which gets multiplied with the gradients.

As already said in the introduction of this section, the goal of the Deep Dream technique is not to minimize the cost function, but to modify the input to maximize the activations of certain neurons.
Concrete one can choose a whole layer within the net or just some neurons of a layer which output should be maximized.
Because the weights and biases are fixed the input image is modified in order to activate the neurons more.
To achieve this an image is fed into the net and the activation values of neurons at a layer gets computed.

%TODO Herausfinden welche loss function verwendet wird

%TODO unsicher
The backward pass will now start from the specified layer to the input.
The gradients gets computed, but instead of adding the gradients to the weights and biases, they get added to the input image.



Same as the normal training process of the network, the dreaming is done by using multiple iteration.
There are also other techniques to improve the results which will be discussed in Section \ref{sec:optimizations}

% TODO: erklären das jitter / image shift dafür da ist, dass übe rmehrere iterationen die änderungen smooth zusammen gehen siehe calc_grad_tiled in https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/tutorials/deepdream/deepdream.ipynb


% TODO: Das ist eher eine Konklusion und sollte später bei der Auswertung erwähnt werden


% Niedrige Layer enthalten eher abstrakte Features we Linien, Kurven und Ecken während höhere Layer konkrete Features wie z.B. Augen oder Säulen abbilden



% https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/tutorials/deepdream/deepdream.ipynb
% Achtung Zitat: 
%\enquote{It consists of a set of layers that apply a sequence of transformations to the input image. The parameters of these transformations were determined during the training process by a variant of gradient descent algorithm. 
% The internal image representations may seem obscure, but it is possible to visualize and interpret them.}


\subsection{Guided Dreaming}
% mit Reference Image
Instead of increasing the occurrence of present features within an image we can also use a reference image in order to achieve \emph{Guided Dreaming}.
For instance if you use a reference picture with animals, the Deep Dreaming algorithm will add animal features the input image that gets modified.


\subsection{Optimizations}
\label{sec:optimizations}
% step_size
% upscaling nötig, da das Bild auf einer Auflösung irgendwann fast fertig optimiert ist
