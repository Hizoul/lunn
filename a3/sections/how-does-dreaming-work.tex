% https://arxiv.org/pdf/1312.6034v2.pdf - Deep Inside Convolutional Networks: Visualising Image Classification Models and Saliency Maps

% Gradient: Technik um größten Anstieg zu finden -> man nimmt also beim Trainieren den negativen Gradient: -Delta(->W) = [0.2 -1 ...] // GIbt an wie die Weights verändert werden sollten
% Numerische Approximation des Gradients: https://en.wikipedia.org/wiki/Numerical_differentiation
% Gradient Descent/ Full Backpropagation): mache es für alle Trainingsamples und nehme den durchschnitt
% Stochasitic Gradient Descent: Nehm ein paar und nehme den Durchschnitt
% Backpropagation: Determine how one trainign example want to change the weights and biases

\section{How Does Dreaming Work}
\label{sec:how}

This section will focus on the main techniques how dreaming actual work.
Similar to the training process of a network, the dreaming algorithm is an optimization problem.
But rather than training a network to classify samples, one take a pretrained network and modify the input that some neurons in the net are getting more activated.


\subsection{General}
When looking at the training process of a (Convolutional) Neural Network, one can see that the weights and biases are changed in such a way that a given error function gets minimized.
Because the error function, for instance \emph{mean squared error}, has a lot of dependencies to the weights and biases the derivation is very complex if not even infeasible.
That's why the \emph{Gradient descent} algorithm is used to find the (global) minimum\footnote{however, using the gradient descent often leads only to a local minimum} of the function by taking small steps into the direction of the minima.
The partial derivative $\frac{\partial C}{\partial w}$ in respects to the weights	$\frac{\partial C}{\partial b}$ and to the biases of the cost function C is used to calculate how quickly the cost changes when the weights and biases are changed.

So in order to learn a network there are two essential steps:
First there is the \emph{forward} pass, where a sample gets classified by the net.
After that the error made by the network is calculated with the cost function.
The second step, the \emph{backward} pass, consists of the algorithm of the Gradient Descent, often called \emph{backpropagation} in the context of Neural Networks.
Based on result of the error function, also called \emph{loss}, the gradients for every weight and bias is calculated.

The gradients can be interpret as the direction in which the function should step in order to maximize the loss.
Hence we want to minimize the loss we take the negative gradient and add them to the weights and biases.
To prevent too slow learning or \enquote{overshooting} a local minima a \emph{learning rate} is introduced which gets multiplied with the gradients.

%TODO: Vielleicht Layer als l_end beschreiben, damit man dann sowas sagen kann wie: Gradients von Layer 0 - l_end sind berechnet worden
As already said in the introduction of this section, the goal of the Deep Dream algorithm is not to minimize the cost function, but to modify the input to maximize the activations of certain neurons.
Concrete speaking one can choose a whole layer within the net or just some neurons of a layer which output should be maximized according to the L2 norm.
By a normal forward pass to the specified layer the activation values gets computed.
In order to start the backward pass now, the gradient of the specified layer has to be set.
This can be done in several ways, at this point just assume that the gradients are set to the same values as the
activation values of that layer.
By starting the backward pass the backpropagation algorithm composes the gradient of each previous layer to compute the gradient of the whole submodel by automatic differentiation.\footnote{TODO Quelle: \url{http://caffe.berkeleyvision.org/tutorial/forward_backward.html}}
That's why the gradients (or at least the values) of the specified layer has to be set to something, because every layer needs the gradients from its previous layer in order to work.


When the backward pass is done, all gradients are computed.
Obviously the gradients consists of many as values as there are weights and biases.
But with Deep Dream only the weights of the input layer are important, because these can be added to the input image.
By adding the gradients, the activation values at the specified layer will increase and with that, the features will become more present.

\subsection{Optimizations}
\label{sec:optimizations}
Same as the normal training process of the network, the dreaming is done by using multiple iteration.
By repeating the process over and over again, the features become more and more present, obviously only to an extend.

Another thing that is commonly used is the \emph{step size}, also called \emph{learning rate} in Neural Networks.
Instead of just adding the gradients as they are, one can higher or lower the effect by choosing a $step size < 1$ and $> 1$ respectively.

% TODO Octaves
TODO: Besides these two quite simple techniques \emph{octaves} are used. 

Another small tweak is to use a jitter, that shifts the image along the x and y-axis in a random interval before the forward and backward pass.
This will lead to different activation functions and hence to different gradients and finally to a different change to the image.
This jitter shift is undone, after the image is modified.
This small trick not just lead to different results, it also provides smooth transitions between remarkable enhanced features.



\subsection{Guided Dreaming}
\label{guided-dreaming}
Instead of increasing the occurrence of present features within an image one can also use a reference image in order to achieve \emph{Guided Dreaming}.
The approach is basically to do a normal forward pass with a reference image and to save the activations at the specified layer.

Zitat https://github.com/google/deepdream/blob/master/dream.ipynb: \enquote{Instead of maximizing the L2-norm of current image activations, we try to maximize the dot-products between activations of current image, and their best matching correspondences from the guide image.}



For instance if you use a reference picture with animals, the Deep Dreaming algorithm will add animal features the input image that gets modified.

