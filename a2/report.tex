\documentclass{article}[]
\usepackage[textwidth=15cm]{geometry}
\usepackage[table,xcdraw]{xcolor}
\usepackage{graphicx}
\usepackage{listings}
\usepackage{hyperref}
\usepackage{pdfpages}
\usepackage{csvsimple}
\usepackage{float}
\usepackage{csquotes}
\makeatletter
\newcommand\urlfootnote@[1]{\footnote{\url@{#1}}}
\DeclareRobustCommand{\urlfootnote}{\hyper@normalise\urlfootnote@}
\makeatother

\begin{document}
\title{Neural Networks Assignment 2}
\author{Group 35}
\maketitle
\lstset{
  basicstyle=\ttfamily,
  keywordstyle=\bfseries,
  language=Java,
  frame=single,
  aboveskip=11pt,
  belowskip=11pt,
  breaklines=true,
  breakatwhitespace=false,
  showspaces=false,
  showstringspaces=false,
  numbers=left,
  stepnumber=1,    
  firstnumber=1,
  numberfirstline=true
}

\section{Introduction}
This report discusses our results for the second assignment for the Neural Network
course, given in spring 2018 at LIACS.
The main purpose of this assignment is to getting familiar with the neural networks library \textit{Keras}\footnote{https://keras.io/ ; Visited 04.04.2018} written for \textit{python}\footnote{https://python.org/ ; Visited 04.04.2018}.

Section \ref{sec:mnist} focuses on using two different methods, namely \emph{Multilayer perceptron} (MLP) and \emph{Convolutional Neural Network} (CNN), to classify handwritten digits taken from the famous MNIST-Dataset\footnote{http://yann.lecun.com/exdb/mnist/ ; Visited 04.04.2018}.

%TODO
Section \ref{sec:seq} is about natural language processing or the processing of arbitrary sequences / sentences,  required from the second part of assignment 2\footnote{See Slide 23 of "RecurrentNNs.pdf"}. This is done using LSTM's and GRU's instead of plain Recurrent Neural Networks (RNN's)
Section \ref{sec:rnn} then uses plain RNN's to see if based on pure sequence / string input it is possible to teach a neural network to do basic calculations like addition and multiplication.

\section{MNIST-Classification with Keras}
\label{task-1}
% xxxxxxxxx – print top 3 “most misclassified” digits
% xxxxxxxx – how do you define “most misclassified”? 
% xxxxxxxx change the error function to MSE and rerun your experiments
%– to which extent are your results reproducible? In other words:
%what is the impact of “non-controllable” randomness?
%xxxxxxxxx apply mnist_mlp.py and mnist_cnn.py to a “randomly permutedversion” of the mnist dataset (pixels of all images permuted with help of a fixed random permutation of their indices)
\label{sec:mnist}
This section is about the classification of handwritten digits using the MNIST-Dataset.
For this we used existing \textit{Keras}-example codes called \textit{mnist\_mlp.py}\footnote{\url{https://github.com/keras-team/keras/blob/master/examples/mnist_mlp.py} ; Visited 04.04.2018} and \textit{mnist\_cnn.py}\footnote{\url{https://github.com/keras-team/keras/blob/master/examples/mnist_cnn.py} ; Visited 04.04.2018}.
In section \ref{example-scripts} we describe the model of these two scripts.


First we want to take a look at the three \emph{most misclassified digits}, see Section \ref{most-misclassified-digits}.
We define \textit{most misclassified} as follows: The samples where the probability of the $y_{true}$ label was the lowest.

After that we want to determine the impact of the used error function of these two nets.
Both the examples are using the \emph{categorical cross entropy} (CEE) error function.
In order to see how the performance of the nets change we repeated the experiments while using the \emph{mean squared error} function.
The findings are described in Section \ref{evaluation}

For discovering possible weaknesses of the MLP, we permuted both the training and the test samples using a fixed permutation matrix.
After that we run the same experiment again with the CNN and described the results in Section \ref{evaluation}.


\subsection{Example Scripts}
\label{example-scripts}

The model of the \emph{mnist\_mlp.py}, see Listing \ref{mnist-mlp}, script looks like the following:
As you can see, the script builds a sequential model with three \emph{Dense} layers and two \emph{Dropout} layers.
As activation function the \emph{rectified linear unit} is used for the first two Dense layers.
The output layer uses the \emph{softmax} activation function.
The Dropout layers helps preventing overfitting, by dropping samples during the training process.

The MLP net trains for a total of 20 epochs and has 669,706 trainable parameters .

\begin{lstlisting}[language=Python, label=mnist-mlp, caption={mnist\_cnn.py model}, captionpos=b]
model = Sequential()
model.add(Dense(512, activation='relu', input_shape=(784,)))
model.add(Dropout(0.2))
model.add(Dense(512, activation='relu'))
model.add(Dropout(0.2))
model.add(Dense(num_classes, activation='softmax'))
\end{lstlisting}

The sequential model of the \emph{mnist\_cnn.py}, see Listing \ref{mnist-cnn}, is more complicated and contains the following layers:
A convolutional 2D layer with a kernel size of (3, 3), which creates 32 feature maps in the size of (26, 26).
A second convolutional 2D layer with a kernel size of (3, 3), which creates 64 feature maps in the size of (24, 24).
The max pooling 2D layer reduces the size of the feature maps, by finding the highest value in a (2, 2) window and output it to the next layer.
The Flatten layer reduces the dimensionality to 1, in order to use two Dense layers afterwards.
Again the \emph{rectified linear unit} activation function is used everywhere, but in the output layer, where the \emph{softmax} function is used.
Also there are two Dropout layers, to prevent overfitting.

% dropout layers erwähnen
The CNN trains for a total of 12 epochs and has 1,199,882 trainable parameters, which is quite a lot and explains the long runtime.

\begin{lstlisting}[language=Python, label=mnist-cnn, caption={mnist\_mlp.py model}, captionpos=b]
model = Sequential()
model.add(Conv2D(32, kernel_size=(3, 3),
                 activation='relu',
                 input_shape=input_shape))
model.add(Conv2D(64, (3, 3), activation='relu'))
model.add(MaxPooling2D(pool_size=(2, 2)))
model.add(Dropout(0.25))
model.add(Flatten())
model.add(Dense(128, activation='relu'))
model.add(Dropout(0.5))
model.add(Dense(num_classes, activation='softmax'))
\end{lstlisting}



\subsection{Most misclassified digits}
\label{most-misclassified-digits}
We run the examples scripts and tracked the three most misclassified digits of both the CNN and the MLP net.
Looking at figure \ref{fig:cnn} we can that the misclassifications of the CNN mostly seem to revolve around thin digits.
The first 9 is too thin, the 1 is barely visible and the 8 is too thin, too little and contains a noisy line.

\begin{figure}[H]
	\centering
	\includegraphics[width=0.5\linewidth]{img/cnn_categorial_cross_non_perm.png}
	\caption{Top three most misclassified digits with the CNN-CEE net}
	\label{fig:cnn}
\end{figure}

The three most misclassified digits of the MLP, shown in figure \ref{fig:mlp}, stand in stark contrast to that.
The left and the right sample seem to be very clearly identifiable.
One problem might be the thickness given that both samples are quite thick.
The middle one is easily identifiable by a human, but is very thin compared to the others, which might be the reason why it was misclassified.
Of course other reasons could have led to this result as well.

\begin{figure}[H]
	\centering
	\includegraphics[width=0.5\linewidth]{img/mpl_categorial_cross_non_perm.png}
	\caption{Top three most misclassified digits with the MLP-CEE net}
	\label{fig:mlp}
\end{figure}

\subsection{Evaluation}
\label{evaluation}

In table \ref{table-1} you can see the amount of misclassified samples out of a total of 10000 test samples per experiment.
The rows describes the net type and the columns describe the error function and whether the permuted or the original data set was used.
It is evident, that CNN outperforms the MLP.

Both networks perform best with the cross-entropy error function.
The mean-squared error only slightly worsens the results of the MLP.
The CNN however, performs much worse using the mean squared error.

When looking at the results for the permuted data, we can see that it has a much greater impact on the CNN than on the MLP results.
It almost doubles the misclassified samples with the CNN, but doesn't affect the MLP results significantly.
This effect can be explained by the nature of the convolutional filters.
A convolutional filter \enquote{detects} features by looking at multiple information, in this case multiple pixels, at the same time.
By permutation the data, the feature recognition cannot work properly anymore, because it's important which pixels are next to each other.
The permutation will affect the results of the filters in an unpredictable way.

This problem doesn't occur with a plain MLP net, because every pixel is considered individuality.
As long as a pixel is moved to another place consistently over all images, it doesn't affect the behavior at all.


\begin{table}[H]
	\centering
	\begin{tabular}{|l|l|l|l|l|}
		\hline
		& \cellcolor[HTML]{C0C0C0}CEE & \cellcolor[HTML]{C0C0C0}CEE-P & \cellcolor[HTML]{C0C0C0}MSE & \cellcolor[HTML]{C0C0C0}MSE-P \\ \hline
		\cellcolor[HTML]{C0C0C0}MLP & \cellcolor[HTML]{FFFFFF}166 & 178                           & 169                         & 189                           \\ \hline
		\cellcolor[HTML]{C0C0C0}CNN & 77                          & 123                           & 149                         & 221                           \\ \hline
	\end{tabular}
	\caption{Amount of misclassified samples. CEE = cross-entropy error; MSE = mean-squared error, P = permutated sample}
	\label{table-1}
\end{table}

\subsection{Randomness impact}
To determine the possible impact of randomness upon accuracy of the recognition we run same tests again.
The results are shown in table \ref{table-2}.
As you can see, the results don't differ too much from the first runs.
The margin between the misclassified samples seems negligible.


\begin{table}[H]
	\centering
	\begin{tabular}{|l|l|l|l|l|}
		\hline
		& \cellcolor[HTML]{C0C0C0}CEE & \cellcolor[HTML]{C0C0C0}CEE-P & \cellcolor[HTML]{C0C0C0}MSE & \cellcolor[HTML]{C0C0C0}MSE-P \\ \hline
		\cellcolor[HTML]{C0C0C0}MLP & \cellcolor[HTML]{FFFFFF}154 & 160                           & 169                         & 174                           \\ \hline
		\cellcolor[HTML]{C0C0C0}CNN & 88                          & 128                           & 147                         & 216                           \\ \hline
	\end{tabular}
	\caption{Amount of misclassified samples. CEE = cross-entropy error; MSE = mean-squared error, P = permutated sample}
	\label{table-2}
\end{table}

\section{Text Generation}
\label{sec:seq}
This section deals with using  \emph{Long short-term memory} (LSTM) networks for natural language processing.
It presents the generation of text (section \ref{sec:gen}) based on the learned data.
The text generation is based on the \textit{Keras}-example called \textit{lstm\_text\_generation.py}\footnote{\url{https://github.com/keras-team/keras/blob/master/examples/lstm_text_generation.py} ; Visited 04.04.2018}.


\subsection{Script}
In this script a Nietzsche writing is used, the text is read and all $distinct\_characters$ are collected and enumerated.
After that the text is split up into \enquote{sentences}.
Sentences is in quotes, because the script uses the \emph{sliding window} technique.
This means we have a window of $window\_size$ characters, this window will be moved in every iteration by $step\_size$ characters.
This results in cut off words, but improves the contextualization between the sentences.
To feed the sentences to the network they are vectorized into a boolean representation.
The encoded data looks has the following shape:  $x=(sentence\_count, distinct\_character\_count)$ and for $y=(sentence\_count, sentence\_length, distinct\_character\_count)$.

This data is then fed to the model shown in Listing \ref{textgen}.
At the end of every epoch, it generates text based on a random text window of the original input text.
Using this \enquote{seed} it will predict character by character.
This happens with a varying degree of diversity.
This means not using the most probable next character, but varying and hence enabling less probable characters to be used, if diversity increases.
According to the author of the script, after 20 epochs text should start to sound coherent.
It also states, that increasing the size of the trained text should improve results.

\subsection{Experiments}
In order to evaluate the generation we run the following experiments
\begin{enumerate}
\item{Run the example-script as is}
\item{Run the example-script as is with increased sliding window size}
\item{Run the example with different text input}
\item{Run the example with different text input and use a GRU instead of LSTM}
\end{enumerate}

Our different text input are all books from Harry Potter combined.
The found book text was quite noisy, so we decided to clean it via a script.
We removed several orthography related characters and new lines.
This resulted in a $distinct\_characters$ drop of 37.
To compare, Nietzsche's text contains 52.
This should improve the results, given that less non fitting characters can be predicted.

In addition we increased the sentence length to 100 instead of 40.
We set the $step\_size$ from 3 to 6.
We decided to adjust these parameters, because 40 characters seem to be too short.
In order to find a good length we looked up the average sentence length in the English language which is supposed to be 15-20 words\footnote{\url{http://countwordsworth.com/blog/what-is-a-good-average-sentence-length/} ; Visited 04.04.2018}.
Also the average word length is approximately 5 characters\footnote{@article{shannon1951prediction,
title={Prediction and entropy of printed English},
author={Shannon, Claude E},
journal={Bell Labs Technical Journal},
volume={30},
number={1},
pages={50--64},
year={1951},
publisher={Wiley Online Library}
}}.
Combining that results in 100 characters per sentence ($20 * 5$).
We also expect a longer sequence length to improve the generated result, given that LSTM's use historical data well, and short sequences might already have dropped relevant previous context\footnote{\url{http://colah.github.io/posts/2015-08-Understanding-LSTMs/}; Visited 04.04.2018}.



\subsection{Results}
% Nietzsche TODO
The script as is generates somewhat valid English words after 60 epochs of learning with a low diversity, see listing \ref{nietzsche-low-diverse}.

When comparing the results of the vanilla script,  with the ones of the adjusted script, see listing \ref{nietzsche-low-diverse-increased}, we can see that increasing the sliding window length does not seem to negatively affect the validity of the generated text. (TODO: schauen obs sogar besser geworden ist)

However, the produced words only makes sense individually.
They do not create meaningful sentences.
When increasing the diversity the results are getting worse and worse.

% Harry Potter Vanilla (sliding window angepasst)
Generated text with low diversity based on the Harry Potter dataset can be found in listing \ref{harry-all-low-diverse}.
The results are very disappointing, given that the training text is much bigger and even cleaned.
We are not able to identify why the increased text corpus size did not improve the results.
But still, the produced text contains at least some valid English words.
Increasing the diversity here only leads to more gibberish sentences or even no valid words at all.

% GRU is supposed to yield to similar results as the LSTM, we found that to be true
% Harry mit Gru
Because of the disappointing results of the LSTM, we replaced it with a GRU.
The GRU is a bit simpler than the LSTM, but should yield similar results \footnote{\url{http://colah.github.io/posts/2015-08-Understanding-LSTMs/}}.
But again it produces largely garbage.
You can find produced text in the listing \ref{evaluation}.
From a subjective viewpoint, it seems to us, that  the GRU has slightly better results than the LSTM.

\section{Text Translation}
\label{sec:trans}
In this section we want to focus on the translation of strings from a source to a target language.
For this we use the \emph{Keras} example script called \emph{lstm\_seq2seq} \footnote{TODO}.

\subsection{Script}
It reads in a text file in which every line represents a translation.
The first part is the source language, delimited by a tab character indicating the start of the target language.
The input gets processed in a similar way as the text generation script, see \ref{TODO}.
One of the changes is, that the representation of the sentences is encoded with floats instead of booleans.

The model consists of an encoder and a decoder model.
Both are LSTMs and are combined into one model in order to train the net.

In total the model trains for 100 epochs.

\subsection{Experiments}
In order to evaluate the translation we run the following experiments

\begin{enumerate}
	\item{Run the example-script as is}
	\item{Run the example with English to Dutch translation}
\end{enumerate}

\subsection{Results}
We validate the translation for correctness by comparing the expectation with the prediction.
We also check against the full dataset.
Source sentences can occur multiple times, hence for these we check if one of the possible translation matches.

% TODO: Fra -> Eng


% English -> Dutch


%s2snl_main Results
%Got 3 out of selected 16
%Got 1318 out of 10000


\section{Learning to calculate from Sequences using RNN}
\label{sec:rnn}

The original example script \textit{addition\_rnn.py}\footnote{\url{https://raw.githubusercontent.com/keras-team/keras/master/examples/addition_rnn.py}; Visited on 16.04.2018} tries to teach a Recurrent Neural Network (RNN) to do simple additions by solely providing sequence to sequence string data.
We modified it to do a multiplication, and generate around 270k entries. The training process is quite lengthy, having reached epoch 17 / 35 after 43 hours our Job got cancelled due to a reboot.
We decided 15 Epochs will have to do and saved the model via keras model.save function. This turned out to be a smart decision because after successfull training, a unicode character exception flew which we were able to debug quickly through the pretrained model.

\appendix
\section{Text Generation}
\begin{lstlisting}[label=nietzsche-low-diverse, caption={Nietzsche after 60 epochs}]
----- diversity: 0.2
TODO
\end{lstlisting}

\begin{lstlisting}[label=lstm-harry-all-low-diverse, caption={LSTM Harry Potter after 60 epochs, diversity=0.2}]
----- diversity: 0.2
----- Generating with seed: "his last, comforting thought before he fell asleep was that even if the potters were involved, there"
his last, comforting thought before he fell asleep was that even if the potters were involved, there
b a t ood the t   the cyctean the l they fot s the and fun w  and and the thu mbled we wh   the orn had back and of t wever me wan the the d evon thoug be t out the abrtu whand the shou harry the the breger po i o  v a anow  ht wan the to has the duull tall war of the habe was be ad  w of his rr t h omn r t sh o t the to  blo th hupp l the the t to to te  s  t  o  thuw  has   t a t  tath harry
\end{lstlisting}

\begin{lstlisting}[label=gru-harry-all-low-diverse, caption={GRU Harry Potter after 60 epochs, diversity=0.2}]
----- diversity: 0.2
----- Generating with seed: "his last, comforting thought before he fell asleep was that even if the potters were involved, there"
his last, comforting thought before he fell asleep was that even if the potters were involved, there
a   af he ain alual thereery he  is aid ash her her h d h the led the he an si  he  as ae a  es of sthe w there ae t  the h as eher the alo ha i as a t he wating of hin  sac o ly e agurene thethe an thuerere theauther the he  harerer the ase hef ther her as the hanou the ther the the at  ithe  the to  erer and as erer there a w ase a her d e thathery the asi he a s thet whe ene se an  s  o he h he
\end{lstlisting}

\end{document}